liste_isbn = [
    ("978-972-0-72746-6", "9789720727466"),
    ("978-972-0-72668-1", "9789720726681"),
    ("978-972-0-72700-8", "9789720727008"),
    ("978-972-0-20691-6", "9789720206916"),
    ("78-972-680-570-0", "789726805700"),
    ("978-989-767-207-1", "9789897672071"),
    ("978-972-0-72699-5", "9789720726995"),
    ("978-972-0-33603-3", "9789720336033"),
    ("978-972-0-72675-9", "9789720726759"),
    ("978-972-627-744-6", "9789726277446"),
    ("978-972-0-70349-1", "9789720703491"),
    ("978-972-0-04479-2", "9789720044792"),
    ("978-972-0-72709-1", "9789720727091"),
    ("978-972-37-2089-1", "9789723720891"),
    ("978-972-0-41006-1", "9789720410061"),
    ("978-972-0-72747-3", "9789720727473"),
    ("978-972-0-72715-2", "9789720727152"),
    ("978-972-0-72726-8", "9789720727268"),
    ("978-972-0-72727-5", "9789720727275"),
    ("978-972-0-42502-7", "9789720425027"),
    ("978-989-744-318-3", "9789897443183"),
    ("978-972-0-04857-8", "9789720048578"),
    ("978-972-0-72625-4", "9789720726254"),
    ("978-972-0-04491-4", "9789720044914"),
    ("978-972-0-72637-7", "9789720726377"),
    ("978989-722-179-8", "9789897221798"),
    ("978-972-0-72621-6", "9789720726216"),
    ("978-972-0-72626-1", "9789720726261"),
    ("978-972-0-04709-0", "9789720047090"),
    ("978-972-0-72635-3", "9789720726353"),
    ("978-972-0-72633-9", "9789720726339"),
    ("978-972-0-72632-2", "9789720726322"),
    ("978-972-0-72748-0", "9789720727480"),
    ("978-972-0-01820-5", "9789720018205"),
    ("978-972-0-03286-7", "9789720032867"),
    ("978-2-36157-223-5", "9782361572235"),
    ("978-972-0-04728-1", "9789720047281"),
    ("9789-722-028-981", "9789722028981"),
    ("978-972-0-04962-9", "9789720049629"),
    ("978-972-0-07151-4", "9789720071514"),
    ("978-972-0-70949-3", "9789720709493"),
    ("978-972-0-71898-3", "9789720718983"),
    ("978-989-676-241-4", "9789896762414"),
    ("978-972-0-71882-2", "9789720718822"),
    ("978-989-676-143-1", "9789896761431"),
    ("978-972-0-72857-9", "9789720728579"),
    ("978-972-0-70613-3", "9789720706133"),
    ("978-972-0-03046-7", "9789720030467"),
    ("978-972-37-2127-0", "9789723721270"),
    ("978-972-37-1982-6", "9789723719826"),
    ("978-972-38-2922-8", "9789723829228"),
    ("978-972-0-70193-0", "9789720701930"),
    ("978-972-0-01866-3", "9789720018663"),
    ("978-972-0-04949-0", "9789720049490"),
    ("978-972-37-1892-8", "9789723718928"),
    ("978-972-0-04650-5", "9789720046505"),
    ("978-972-0-04632-1", "9789720046321"),
    ("978-972-0-70639-3", "9789720706393"),
    ("978-972-0-17152-8", "9789720171528"),
    ("978-972-0-70938-7", "9789720709387"),
    ("978-972-0-17153-5", "9789720171535"),
    ("978-972-37-1904-8", "9789723719048"),
    ("978-972-0-11305-4", "9789720113054"),
    ("978-972-0-70829-8", "9789720708298"),
    ("978-972-0-11306-1", "9789720113061"),
    ("978-972-0-17079-8", "9789720170798"),
    ("978-972-0-72595-0", "9789720725950"),
    ("978-972-0-70999-8", "9789720709998"),
    ("978-972-0-71604-0", "9789720716040"),
    ("978-972-0-72720-6", "9789720727206"),
    ("978-972-37-0656-7", "9789723706567"),
    ("978-972-0-01274-6", "9789720012746"),
    ("978-972-0-72706-0", "9789720727060"),
    ("978-972-0-02010-9", "9789720020109"),
    ("9789722066464-", "9789722066464"),
    ("978-972-0-17150-4", "9789720171504"),
    ("978-972-0-40148-9", "9789720401489"),
    ("978-972-0-11024-4", "9789720110244"),
    ("978-2-906462-69-4", "9782906462694"),
    ("978-972-680-334-8", "9789726803348"),
    ("978-989-676-152-3", "9789896761523"),
    ("978-972-0-41018-4", "9789720410184"),
    ("978-972-0-72972-9", "9789720729729"),
    ("978-972-0-70909-7", "9789720709097"),
    ("978-989-767-259-0", "9789897672590"),
    ("978-972-0-70780-2", "9789720707802"),
    ("978-989-767-206-4", "9789897672064"),
    ("978-972-0-71899-0", "9789720718990"),
    ("978-972-0-00104-7", "9789720001047"),
    ("978-3-946277-16-3", "9783946277163"),
    ("978-972-0-72749-7", "9789720727497"),
    ("978-2-906462-99-1", "9782906462991"),
    ("978-972-0-72822-7", "9789720728227"),
    ("978-972-0-01506-8", "9789720015068"),
    ("978-972-0-71884-6", "9789720718846"),
    ("978-972-0-72688-9", "9789720726889"),
    ("978-972-0-72849-4", "9789720728494"),
    ("978-972-0-04739-7", "9789720047397"),
    ("978-989-8093-66-0", "9789898093660"),
    ("978-972-37-0983-4", "9789723709834"),
    ("978-972-0-04494-5", "9789720044945"),
    ("978-972-0-71894-5", "9789720718945"),
    ("978-972-0-78661-6", "9789720786616"),
    ("978-972-0-71878-5", "9789720718785"),
    ("978-972-0-70745-1", "9789720707451"),
    ("978-972-0-72690-2", "9789720726902"),
    ("978-989-20-8279-0", "9789892082790"),
    ("978-972-37-2145-4", "9789723721454"),
    ("978-972-0-72674-2", "9789720726742"),
    ("978-972-37-1947-5", "9789723719475"),
    ("978-972-0-72689-6", "9789720726896"),
    ("978-972-0-72703-9", "9789720727039"),
    ("978-972-0-72684-1", "9789720726841"),
    ("978-972-0-72841-8", "9789720728418"),
    ("978-972-0-03364-2", "9789720033642"),
    ("978-972-0-72679-7", "9789720726797"),
    ("972-99084-0-0", "9729908400"),
    ("978-989-886-855-8", "9789898868558"),
    ("978-972-0-20557-5", "9789720205575"),
    ("978-972-0-72677-3", "9789720726773"),
    ("978-972-0-70927-1", "9789720709271"),
    ("978-972-0-72614-8", "9789720726148"),
    ("978-972-0-03365-9", "9789720033659"),
    ("978-972-0-70936-3", "9789720709363"),
    ("978-972-0-03302-4", "9789720033024"),
    ("978-972-0-72978-1", "9789720729781"),
    ("978-972-0-72955-2", "9789720729552"),
    ("978-972-0-20679-4", "9789720206794"),
    ("978-972-0-03194-5", "9789720031945"),
    ("978-972-0-04952-0", "9789720049520"),
    ("978-972-0-03101-3", "9789720031013"),
    ("978-972-37-0795-3", "9789723707953"),
    ("978-972-37-1689-4", "9789723716894"),
    ("978-972-0-04733-5", "9789720047335"),
    ("978-972-0-03192-1", "9789720031921"),
    ("978-972-0-07144-6", "9789720071446"),
    ("978-972-0-70638-6", "9789720706386"),
    ("978-972-0-70777-2", "9789720707772"),
    ("978-972-0-72856-2", "9789720728562"),
    ("978-972-0-04881-3", "9789720048813"),
    ("978-2-36732-190-5", "9782367321905"),
    ("978-972-37-1905-5", "9789723719055"),
    ("978-972-0-72874-6", "9789720728746"),
    ("978-972-0-70044-5", "9789720700445"),
    ("978-972-0-04695-6", "9789720046956"),
    ("978-972-0-31983-8", "9789720319838"),
    ("978-989-809-382-0", "9789898093820"),
    ("978-972-0-72694-0", "9789720726940"),
    ("978-972-0-72707-7", "9789720727077"),
    ("978-972-0-72657-5", "9789720726575"),
    ("978-972-0-03147-1", "9789720031471"),
    ("978-972-0-72622-3", "9789720726223"),
    ("978-972-0-70176-3", "9789720701763"),
    ("978-2-36732-145-5", "9782367321455"),
    ("978-972-0-70191-6", "9789720701916"),
    ("978-972-0-04747-2", "9789720047472"),
    ("978-972-0-72667-4", "9789720726674"),
    ("978-972-0-73001-5", "9789720730015"),
    ("978-972-0-71007-9", "9789720710079"),
    ("978-972-0-72982-8", "9789720729828"),
    ("978-972-0-72981-1", "9789720729811"),
    ("978-972-0-70238-8", "9789720702388"),
    ("978-972-0-71170-0", "9789720711700"),
    ("978-972-0-03404-5", "9789720034045"),
    ("978-972-0-03137-2", "9789720031372"),
    ("978-972-37-2091-4", "9789723720914"),
    ("978-989-665692-8", "9789896656928"),
    ("978-972-0-70237-1", "9789720702371"),
    ("978-972-37-2156-0", "9789723721560"),
    ("978-972-37-2120-1", "9789723721201"),
    ("9789727932160-", "9789727932160"),
    ("78-989-744-291-9", "789897442919"),
]


# isbn_list = ["9789722230896", "9789720049759", "9789720049773"]
# isbn_list = ["9789720049773"]
import logging
import os
import io
import requests
from PIL import Image
import tempfile
from selenium.webdriver.common.keys import Keys
from django.db import IntegrityError
from selenium import webdriver
from selenium.webdriver.chrome.options import Options
from selenium.webdriver.common.by import By
from core.models import Livre, ImageLivre

from bs4 import BeautifulSoup
from datetime import datetime, timedelta
import locale
import re, json
import time
from django.core.files import File

import os
import pandas as pd
from google_images_download import google_images_download
from sqlite3 import Error
import sqlite3
import os
import boto3
import uuid
from botocore.exceptions import NoCredentialsError
import botocore
from selenium.webdriver.common.by import By
import pathlib
import psycopg2
import logging
from django.template.defaultfilters import slugify
from selenium.common.exceptions import (
    NoSuchElementException,
    WebDriverException,
    ElementNotVisibleException,
    ElementClickInterceptedException,
    ElementNotInteractableException,
)
from selenium.webdriver.firefox.firefox_binary import FirefoxBinary
from selenium.webdriver import FirefoxProfile, Firefox
from selenium.webdriver.common.desired_capabilities import DesiredCapabilities


def make_webdriver():
    options = Options()
    # options.add_argument('--headless')
    options.add_argument("--disable-gpu")
    options.add_argument("--no-sandbox")
    options.add_argument(
        "--user-agent=Mozilla/5.0 (Windows Phone 10.0; Android 4.2.1; Microsoft; Lumia 640 XL LTE) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/42.0.2311.135 Mobile Safari/537.36 Edge/12.10166"
    )
    options.add_argument(
        "user-data-dir = /home/ubuntu/.config/google-chrome/Default"
    )  # Extract this path from "chrome://version/"
    driver = webdriver.Chrome("/usr/bin/chromedriver", chrome_options=options)
    return driver


def make_firefox_webdriver():
    cap = DesiredCapabilities().FIREFOX
    cap["marionette"] = False
    profile = FirefoxProfile("/home/hugo/.mozilla/firefox/19y53y6i.default/")
    profile.set_preference("browser.download.folderList", 2)
    profile.set_preference("browser.download.manager.showWhenStarting", False)
    profile.set_preference("browser.download.manager.alertOnEXEOpen", False)
    profile.set_preference("browser.download.manager.closeWhenDone", False)
    profile.set_preference("browser.download.manager.focusWhenStarting", False)
    profile.set_preference(
        "browser.helperApps.neverAsk.saveToDisk", "application/msexcel"
    )
    driver = Firefox()
    return driver


def grab_from_amazon(url, isbn, driver):
    search_query = "site:amazon.com.br " + str(isbn)
    driver.get(url)
    time.sleep(5)
    try:
        driver.find_element("name", "agree").click()
    except NoSuchElementException:
        pass
    time.sleep(0.5)
    try:
        retry = driver.find_element("name", "p").send_keys(search_query)
    except NoSuchElementException:
        pass
    time.sleep(0.5)
    time.sleep(0.5)
    try:
        driver.find_element("name", "p").send_keys(Keys.RETURN)
    except NoSuchElementException:
        pass
    time.sleep(2)
    soup = BeautifulSoup(driver.page_source, features="html.parser")
    try:
        cites = soup.find_all("a", class_="s-title")
    except NoSuchElementException:
        pass
    output = ""

    if len(cites) > 0:
        if len(cites[0].attrs["href"]) > 0:
            driver.get(cites[0].attrs["href"])
            soup = BeautifulSoup(driver.page_source, features="html.parser")
            desc = soup.find(id="main-image")
            desc_p = ""
            if desc is not None and len(desc.get("src", "")) > 0:
                desc_p = desc["src"]
            return desc_p
    else:
        try:
            driver.get("https://www.google.com")
            time.sleep(5)
            driver.find_element(By.TAG_NAME, "input").send_keys(search_query)
            time.sleep(0.5)
            driver.find_element(By.TAG_NAME, "input").send_keys(Keys.RETURN)
            try:
                # driver.find_elements_by_tag_name("input")
                # driver.find_element(By.XPATH, '//li/a/input').click()
                el = driver.find_elements(By.CSS_SELECTOR, ".box form")
                if el is not None:
                    try:
                        el[0].click()
                    except IndexError:
                        pass
            except NoSuchElementException:
                pass
            soup = BeautifulSoup(driver.page_source, features="html.parser")
            cites = soup.find_all("a")
            src = []
            for cite in cites:
                src.append(cite.attrs["href"])
            p = []
            for s in src:
                if s.find("/url?q", 0, 6) == 0 and ("google.com" not in s):
                    p.append(s)
            if len(p) != 0:
                driver.get("https://www.google.com" + p[0])
                time.sleep(5)
                soup = BeautifulSoup(driver.page_source, features="html.parser")
                desc = soup.find(id="main-image")
                desc_p = ""
                if desc is not None and len(desc.get("src", "")) > 0:
                    desc_p = desc["src"]
                return desc_p
            else:
                driver.get(url)

        except NoSuchElementException:
            pass


def grab_from_fnac(url, isbn, driver):
    search_query = "site:fnac.pt " + str(isbn)
    driver.get(url)
    try:
        driver.find_element("name", "agree").click()
    except NoSuchElementException:
        pass
    time.sleep(0.5)
    try:
        search_query = driver.find_element("name", "p").send_keys(search_query)
    except NoSuchElementException:
        pass
    time.sleep(0.5)
    try:
        driver.find_element("name", "p").send_keys(Keys.RETURN)
    except NoSuchElementException:
        pass
    time.sleep(5)
    soup = BeautifulSoup(driver.page_source, features="html.parser")
    try:
        cites = soup.find_all("a", class_="s-title")
    except NoSuchElementException:
        pass
    output = ""
    # desc['src']
    if len(cites) > 0:
        if len(cites[0].attrs["href"]) > 0:
            driver.get(cites[0].attrs["href"])
            soup = BeautifulSoup(driver.page_source, features="html.parser")
            desc = soup.find("img", attrs={"class": "f-carousel__img"})
            desc_p = ""
            if desc is not None and len(desc.get("src", "")) > 0:
                desc_p = desc["src"]
            return desc_p
    else:
        return ""


def grab_from_bertrand(url, isbn, driver):
    search_query = "site:bertrand.pt " + str(isbn)
    time.sleep(2.5)
    driver.get(url)
    try:
        driver.find_element(By.ID, "scroll-down-btn").click()
    except NoSuchElementException:
        pass
    try:
        time.sleep(0.5)
        driver.find_element("name", "agree").click()
    except NoSuchElementException:
        pass
    time.sleep(0.5)
    try:
        retry = driver.find_element("name", "p").send_keys(search_query)
    except NoSuchElementException:
        pass
    time.sleep(0.5)
    time.sleep(0.5)
    try:
        driver.find_element("name", "p").send_keys(Keys.RETURN)
    except NoSuchElementException:
        pass
    time.sleep(5)
    soup = BeautifulSoup(driver.page_source, features="html.parser")
    try:
        cites = soup.find_all("a", class_="s-title")
    except NoSuchElementException:
        pass
    output = ""
    if len(cites) > 0:
        if len(cites[0].attrs["href"]) > 0:
            driver.get(cites[0].attrs["href"])
            try:
                driver.find_element(By.CLASS_NAME, "gpe-cookies-accept").click()
            except:
                ElementClickInterceptedException
            soup = BeautifulSoup(driver.page_source, features="html.parser")
            desc = soup.find(id="productPageLeftSectionTop-images")
            desc_p = ""
            if desc != None and len(desc.find("img")) is not None:
                desc_p = desc.find("img")["src"]
            return desc_p
    else:
        try:
            driver.get("https://www.google.com")
            time.sleep(5)
            driver.find_element(By.TAG_NAME, "input").send_keys(search_query)
            time.sleep(0.5)
            driver.find_element(By.TAG_NAME, "input").send_keys(Keys.RETURN)
            try:
                # driver.find_elements_by_tag_name("input")
                # driver.find_element(By.XPATH, '//li/a/input').click()
                el = driver.find_elements(By.CSS_SELECTOR, ".box form")
                if el is not None:
                    try:
                        el[0].click()
                    except IndexError:
                        pass
            except NoSuchElementException:
                pass
            soup = BeautifulSoup(driver.page_source, features="html.parser")
            cites = soup.find_all("a")
            src = []
            for cite in cites:
                src.append(cite.attrs["href"])
            p = []
            for s in src:
                if s.find("/url?q", 0, 6) == 0 and ("google.com" not in s):
                    p.append(s)
            if len(p) != 0:
                driver.get("https://www.google.com" + p[0])
                time.sleep(5)
                soup = BeautifulSoup(driver.page_source, features="html.parser")
                desc = soup.find(id="productPageLeftSectionTop-images")
                desc_p = ""
                if desc != None and len(desc.find("img")) is not None:
                    desc_p = desc.find("img")["src"]
                return desc_p
            else:
                return ""

        except NoSuchElementException:
            pass


def grab_from_franceloisir(url, isbn, driver):
    search_query = "site:franceloisirs.com " + str(isbn)
    time.sleep(2.5)
    driver.get(url)
    try:
        driver.find_element(By.ID, "scroll-down-btn").click()
    except NoSuchElementException:
        pass
    try:
        time.sleep(0.5)
        driver.find_element("name", "agree").click()
    except NoSuchElementException:
        pass
    time.sleep(0.5)
    try:
        retry = driver.find_element("name", "p").send_keys(search_query)
    except NoSuchElementException:
        pass
    time.sleep(0.5)
    time.sleep(0.5)
    try:
        driver.find_element("name", "p").send_keys(Keys.RETURN)
    except NoSuchElementException:
        pass
    time.sleep(5)
    soup = BeautifulSoup(driver.page_source, features="html.parser")
    try:
        cites = soup.find_all("a", class_="s-title")
    except NoSuchElementException:
        pass
    output = ""
    if len(cites) > 0:
        if len(cites[0].attrs["href"]) > 0:
            driver.get(cites[0].attrs["href"])
            try:
                driver.find_element(By.ID, "didomi-notice-agree-button").click()
                soup = BeautifulSoup(driver.page_source, features="html.parser")
                desc = soup.find("img", attrs={"class": "bigpic"})
                desc_p = ""
                if desc is not None and len(desc.get("src", "")) > 0:
                    desc_p = desc["src"].replace("//www", "www")
                return desc_p
            except NoSuchElementException:
                pass
    else:
        try:
            driver.get("https://www.google.com")
            time.sleep(5)
            driver.find_element(By.TAG_NAME, "input").send_keys(search_query)
            time.sleep(0.5)
            driver.find_element(By.TAG_NAME, "input").send_keys(Keys.RETURN)
            try:
                # driver.find_elements_by_tag_name("input")
                # driver.find_element(By.XPATH, '//li/a/input').click()
                el = driver.find_elements(By.CSS_SELECTOR, ".box form")
                if el is not None:
                    try:
                        el[0].click()
                    except IndexError:
                        pass
            except NoSuchElementException:
                pass
            soup = BeautifulSoup(driver.page_source, features="html.parser")
            cites = soup.find_all("a")
            src = []
            for cite in cites:
                src.append(cite.attrs["href"])
            p = []
            for s in src:
                if s.find("/url?q", 0, 6) == 0 and ("google.com" not in s):
                    p.append(s)
            if len(p) != 0:
                driver.get("https://www.google.com" + p[0])
                time.sleep(5)
                soup = BeautifulSoup(driver.page_source, features="html.parser")
                desc = soup.find("img", attrs={"class": "bigpic"})
                desc_p = ""
                if desc is not None and len(desc.get("src", "")) > 0:
                    desc_p = desc["src"].replace("//www", "www")
                return desc_p
            else:
                return ""

        except NoSuchElementException:
            pass


def grab_from_wook(url, isbn, driver):
    search_query = "site:wook.pt " + str(isbn)
    driver.get(url)
    time.sleep(2.5)
    try:
        time.sleep(0.5)
        driver.find_element("name", "agree").click()
    except NoSuchElementException:
        pass
    time.sleep(0.5)
    try:
        retry = driver.find_element("name", "p").send_keys(search_query)
    except NoSuchElementException:
        pass
    time.sleep(0.5)
    time.sleep(0.5)
    try:
        driver.find_element("name", "p").send_keys(Keys.RETURN)
    except NoSuchElementException:
        pass
    time.sleep(5)
    soup = BeautifulSoup(driver.page_source, features="html.parser")
    try:
        cites = soup.find_all("a", class_="s-title")
    except NoSuchElementException:
        pass
    output = ""
    if len(cites) > 0:
        if len(cites[0].attrs["href"]) > 0:
            driver.get(cites[0].attrs["href"])
            soup = BeautifulSoup(driver.page_source, features="html.parser")
            try:
                driver.find_element(By.CLASS_NAME, "gpe-cookies-accept").click()
            except NoSuchElementException:
                pass
            desc = soup.find("img", attrs={"class": "img-responsive"})
            desc_p = ""
            if desc is not None and len(desc.get("data-src", "")) > 0:
                desc_p = desc["data-src"]
            return desc_p.split(",")[0].split(" ")[0]
    else:
        try:
            driver.get("https://www.google.com")
            time.sleep(5)
            driver.find_element(By.TAG_NAME, "input").send_keys(search_query)
            time.sleep(0.5)
            driver.find_element(By.TAG_NAME, "input").send_keys(Keys.RETURN)
            try:
                # driver.find_elements_by_tag_name("input")
                # driver.find_element(By.XPATH, '//li/a/input').click()
                el = driver.find_elements(By.CSS_SELECTOR, ".box form")
                if el is not None:
                    try:
                        el[0].click()
                    except IndexError:
                        pass
            except NoSuchElementException:
                pass
            soup = BeautifulSoup(driver.page_source, features="html.parser")
            cites = soup.find_all("a")
            src = []
            for cite in cites:
                src.append(cite.attrs["href"])
            p = []
            for s in src:
                if s.find("/url?q", 0, 6) == 0 and ("google.com" not in s):
                    p.append(s)
            if len(p) != 0:
                driver.get("https://www.google.com" + p[0])
                time.sleep(5)
                soup = BeautifulSoup(driver.page_source, features="html.parser")
                desc = soup.find("img", attrs={"class": "img-responsive"})
                desc_p = ""
                if desc is not None and len(desc.get("data-src", "")) > 0:
                    desc_p = desc["data-src"]
                return desc_p.split(",")[0].split(" ")[0]

        except NoSuchElementException:
            pass


def grab_from_decitre(url, isbn, driver):
    search_query = "site:decitre.fr " + str(isbn)
    driver.get(url)
    time.sleep(2.5)
    try:
        driver.find_element(By.ID, "scroll-down-btn").click()
    except NoSuchElementException:
        pass
    try:
        time.sleep(0.5)
        driver.find_element("name", "agree").click()
    except NoSuchElementException:
        pass
    time.sleep(0.5)
    try:
        retry = driver.find_element("name", "p").send_keys(search_query)
    except NoSuchElementException:
        pass
    time.sleep(0.5)
    time.sleep(0.5)
    try:
        driver.find_element("name", "p").send_keys(Keys.RETURN)
    except NoSuchElementException:
        pass
    time.sleep(5)
    soup = BeautifulSoup(driver.page_source, features="html.parser")
    try:
        cites = soup.find_all("a", class_="s-title")
    except NoSuchElementException:
        pass
    output = ""
    if len(cites) > 0:
        if len(cites[0].attrs["href"]) > 0:
            driver.get(cites[0].attrs["href"])
            try:
                driver.find_element(By.ID, "onetrust-accept-btn-handler").click()
            except NoSuchElementException:
                pass
            soup = BeautifulSoup(driver.page_source, features="html.parser")
            desc = soup.find("picture", attrs={"class": "lozad lozad--loaded"})
            desc_p = ""
            if (
                desc is not None
                and len(desc.get("data-iesrc", "")) > 0
                and desc["data-iesrc"]
                != "https://products-images.di-static.com/image/rachid-benzine-les-silences-des-peres/9782021477764-200x303-1.jpg"
            ):
                desc_p = desc["data-iesrc"]
            return desc_p
    else:
        try:
            driver.get("https://www.google.com")
            time.sleep(5)
            driver.find_element(By.TAG_NAME, "input").send_keys(search_query)
            time.sleep(0.5)
            driver.find_element(By.TAG_NAME, "input").send_keys(Keys.RETURN)
            try:
                # driver.find_elements_by_tag_name("input")
                # driver.find_element(By.XPATH, '//li/a/input').click()
                el = driver.find_elements(By.CSS_SELECTOR, ".box form")
                if el is not None:
                    try:
                        el[0].click()
                    except IndexError:
                        pass
            except NoSuchElementException:
                pass
            soup = BeautifulSoup(driver.page_source, features="html.parser")
            cites = soup.find_all("a")
            src = []
            for cite in cites:
                src.append(cite.attrs["href"])
            p = []
            for s in src:
                if s.find("/url?q", 0, 6) == 0 and ("google.com" not in s):
                    p.append(s)
            if len(p) != 0:
                driver.get("https://www.google.com" + p[0])
                time.sleep(5)
                soup = BeautifulSoup(driver.page_source, features="html.parser")
                desc = soup.find("picture", attrs={"class": "lozad lozad--loaded"})
                desc_p = ""
                if desc is not None and len(desc.get("data-iesrc", "")) > 0:
                    desc_p = desc["data-iesrc"]
                return desc_p
            else:
                return ""

        except NoSuchElementException:
            pass


def grab_from_eyrolles(url, isbn, driver):
    search_query = "site:eyrolles.com " + str(isbn)
    time.sleep(2.5)
    try:
        driver.find_element(By.ID, "scroll-down-btn").click()
    except NoSuchElementException:
        pass
    try:
        time.sleep(0.5)
        driver.find_element("name", "agree").click()
    except NoSuchElementException:
        pass
    time.sleep(0.5)
    try:
        retry = driver.find_element("name", "p").send_keys(search_query)
    except NoSuchElementException:
        pass
    time.sleep(0.5)
    time.sleep(0.5)
    try:
        driver.find_element("name", "p").send_keys(Keys.RETURN)
    except NoSuchElementException:
        pass
    time.sleep(5)
    soup = BeautifulSoup(driver.page_source, features="html.parser")
    try:
        cites = soup.find_all("a", class_="s-title")
    except NoSuchElementException:
        pass
    output = ""
    if len(cites) > 0:
        if len(cites[0].attrs["href"]) > 0:
            driver.get(cites[0].attrs["href"])
            soup = BeautifulSoup(driver.page_source, features="html.parser")
            desc = soup.find("img", {"class": "w-100"})
            desc_p = ""
            if desc is not None and len(desc.get("src", "")) > 0:
                if desc["src"] != "h":
                    desc_p = desc["src"].replace("//", "https://")
                else:
                    desc_p = desc["src"]
            return desc_p
    else:
        try:
            driver.get("https://www.google.com")
            time.sleep(5)
            driver.find_element(By.TAG_NAME, "input").send_keys(search_query)
            time.sleep(0.5)
            driver.find_element(By.TAG_NAME, "input").send_keys(Keys.RETURN)
            try:
                # driver.find_elements_by_tag_name("input")
                # driver.find_element(By.XPATH, '//li/a/input').click()
                el = driver.find_elements(By.CSS_SELECTOR, ".box form")
                if el is not None:
                    try:
                        el[0].click()
                    except IndexError:
                        pass
            except NoSuchElementException:
                pass
            soup = BeautifulSoup(driver.page_source, features="html.parser")
            cites = soup.find_all("a")
            src = []
            for cite in cites:
                src.append(cite.attrs["href"])
            p = []
            for s in src:
                if s.find("/url?q", 0, 6) == 0 and ("google.com" not in s):
                    p.append(s)
            if len(p) != 0:
                driver.get("https://www.google.com" + p[0])
                time.sleep(5)
                soup = BeautifulSoup(driver.page_source, features="html.parser")
                desc = soup.find("img", {"class": "w-100"})
                desc_p = ""
                if desc is not None and len(desc.get("src", "")) > 0:
                    if desc["src"] != "h":
                        desc_p = desc["src"].replace("//", "https://")
                    else:
                        desc_p = desc["src"]
                return desc_p
            else:
                return ""

        except NoSuchElementException:
            pass


def grab_from_ombresblanches(url, isbn, driver):
    search_query = "site:ombres-blanches.fr " + str(isbn)
    driver.get(url)
    try:
        driver.find_element("name", "agree").click()
    except NoSuchElementException:
        pass
    time.sleep(0.5)
    try:
        retry = driver.find_element("name", "p").send_keys(search_query)
    except NoSuchElementException:
        pass
    time.sleep(0.5)
    try:
        driver.find_element("name", "p").send_keys(Keys.RETURN)
    except NoSuchElementException:
        pass
    time.sleep(5)
    soup = BeautifulSoup(driver.page_source, features="html.parser")
    try:
        cites = soup.find_all("a", class_="s-title")
    except NoSuchElementException:
        pass
    output = ""
    if len(cites) > 0:
        if len(cites[0].attrs["href"]) > 0:
            driver.get(cites[0].attrs["href"])
            soup = BeautifulSoup(driver.page_source, features="html.parser")
            desc = soup.find("img", {"class": "test1 imgcouv"})
            desc_p = ""
            if desc is not None and len(desc.get("src", "")) > 0:
                if desc["src"] != "h":
                    desc_p = desc["src"].replace("//", "https://")
                else:
                    desc_p = desc["src"]
            return desc_p
    else:
        try:
            driver.get("https://www.google.com")
            time.sleep(5)
            driver.find_element(By.TAG_NAME, "input").send_keys(search_query)
            time.sleep(0.5)
            driver.find_element(By.TAG_NAME, "input").send_keys(Keys.RETURN)
            try:
                # driver.find_elements_by_tag_name("input")
                # driver.find_element(By.XPATH, '//li/a/input').click()
                el = driver.find_elements(By.CSS_SELECTOR, ".box form")
                if el is not None:
                    try:
                        el[0].click()
                    except IndexError:
                        pass
            except NoSuchElementException:
                pass
            soup = BeautifulSoup(driver.page_source, features="html.parser")
            cites = soup.find_all("a")
            src = []
            for cite in cites:
                src.append(cite.attrs["href"])
            p = []
            for s in src:
                if s.find("/url?q", 0, 6) == 0 and ("google.com" not in s):
                    p.append(s)
            if len(p) != 0:
                driver.get("https://www.google.com" + p[0])
                time.sleep(5)
                soup = BeautifulSoup(driver.page_source, features="html.parser")
                desc = soup.find("img", {"class": "test1 imgcouv"})
                desc_p = ""
                if desc is not None and len(desc.get("src", "")) > 0:
                    if desc["src"] != "h":
                        desc_p = desc["src"].replace("//", "https://")
                    else:
                        desc_p = desc["src"]
                return desc_p
            else:
                return ""

        except NoSuchElementException:
            pass


def grab_from_amazoncom(url, isbn, driver):
    search_query = "site:amazon.com " + str(isbn)
    driver.get(url)
    time.sleep(2.5)
    try:
        driver.find_element(By.ID, "scroll-down-btn").click()
    except NoSuchElementException:
        pass
    try:
        time.sleep(0.5)
        driver.find_element("name", "agree").click()
    except NoSuchElementException:
        pass
    time.sleep(0.5)
    try:
        retry = driver.find_element("name", "p").send_keys(search_query)
    except NoSuchElementException:
        pass
    time.sleep(0.5)
    time.sleep(0.5)
    try:
        driver.find_element("name", "p").send_keys(Keys.RETURN)
    except NoSuchElementException:
        pass
    time.sleep(5)
    soup = BeautifulSoup(driver.page_source, features="html.parser")
    try:
        cites = soup.find_all("a", class_="s-title")
    except NoSuchElementException:
        pass
    output = ""
    if len(cites) > 0:
        if len(cites[0].attrs["href"]) > 0:
            driver.get(cites[0].attrs["href"])
            soup = BeautifulSoup(driver.page_source, features="html.parser")
            desc = soup.find(id="main-image")
            desc_p = ""
            if desc is not None and len(desc.get("src", "")) > 0:
                desc_p = desc["src"]
            return desc_p
    else:
        try:
            driver.get("https://www.google.com")
            time.sleep(5)
            driver.find_element(By.TAG_NAME, "input").send_keys(search_query)
            time.sleep(0.5)
            driver.find_element(By.TAG_NAME, "input").send_keys(Keys.RETURN)
            try:
                # driver.find_elements_by_tag_name("input")
                # driver.find_element(By.XPATH, '//li/a/input').click()
                el = driver.find_elements(By.CSS_SELECTOR, ".box form")
                if el is not None:
                    try:
                        el[0].click()
                    except IndexError:
                        pass
            except NoSuchElementException:
                pass
            soup = BeautifulSoup(driver.page_source, features="html.parser")
            cites = soup.find_all("a")
            src = []
            for cite in cites:
                src.append(cite.attrs["href"])
            p = []
            for s in src:
                if s.find("/url?q", 0, 6) == 0 and ("google.com" not in s):
                    p.append(s)
            if len(p) != 0:
                driver.get("https://www.google.com" + p[0])
                time.sleep(5)
                soup = BeautifulSoup(driver.page_source, features="html.parser")
                desc = soup.find(id="main-image")
                desc_p = ""
                if desc is not None and len(desc.get("src", "")) > 0:
                    desc_p = desc["src"]
                return desc_p
            else:
                return ""

        except NoSuchElementException:
            pass


def download_it(isbn, img_url, placeholderpath):
    filename, file_extension = os.path.splitext(img_url)
    path2file = os.path.join(placeholderpath, str(isbn))
    buffer = tempfile.SpooledTemporaryFile(max_size=1e9)
    # "Host": "img.bertrand.pt",
    headers = {
        "Accept-Language": "en-US,en;q=0.5",
        "Accept-Encoding": "gzip, deflate, br",
        "DNT": "1",
        "Connection": "keep-alive",
        "Cookie": "cf_bm=lCDRea5X.M9rLHBnQzgcjmil4eyON9i0cR7Bcsy.v3w-1692946198-0-Ael/iXcHHo+jOobdv247BFEV9csNb/c9e9Be6WIMasxRHckItFnPioDtoJP9h99uFvMWAPV7MBYxacGnSGFF5I8=; TS01f5daf1=01f2ba636c824a4dd72942774ce04106e6f79a68dc5b9ddb722a26497b2894fa6b64aa4b5243e29e6504ce02269a3c907145241aec",
        "Sec-Fetch-Dest": "document",
        "Sec-Fetch-Mode": "navigate",
        "Sec-Fetch-User": "?1",
        "Pragma": "no-cache",
        "Cache-Control": "no-cache",
        "Accept": "text/html,application/xhtml+xml,application/xml;q=0.9,image/avif,image/webp,image/apng,*/*;q=0.8,application/signed-exchange;v=b3;q=0.7",
        "Cache-Control": "max-age=0",
        "Sec-Ch-Ua": '"Not.A/Brand";v="8", "Chromium";v="114", "Google Chrome";v="114"',
        "Sec-Ch-Ua-Mobile": "?0",
        "Sec-Ch-Ua-Platform": "Windows",
        "Sec-Fetch-Site": "same-origin",
        "Upgrade-Insecure-Requests": "1",
        "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/114.0.0.0 Safari/537.36",
    }
    # proxies = {"http": r"http://user:password@16.170.236.181:8888"}

    r = requests.get(
        img_url,
        stream=True,
        headers=headers,
        # proxies=proxies,
    )
    if r.status_code == 200:
        downloaded = 0
        # filesize = int(r.headers["content-length"])
        for chunk in r.iter_content(chunk_size=1024):
            downloaded += len(chunk)
            buffer.write(chunk)
            # print(downloaded / filesize)
        buffer.seek(0)
        i = Image.open(io.BytesIO(buffer.read()))
        if not os.path.exists(path2file):
            os.mkdir(os.path.join(placeholderpath, str(isbn)))
        i.save(
            os.path.join(placeholderpath, str(isbn), f"{isbn}.{i.format}"), quality=100
        )
        logging.error(f"success: {isbn}: success")
    else:
        logging.error(f"{isbn},{img_url}")
    buffer.close()


def shouldDownloaded(placeholderpath, isbn):
    path2file = os.path.join(placeholderpath, str(isbn))
    if not os.path.exists(path2file):
        os.makedirs(path2file)
        return True
    if (
        len(
            [
                name
                for name in os.listdir(path2file)
                if os.path.isfile(os.path.join(path2file, name))
            ]
        )
        == 0
    ):
        return True
    print("skipping " + isbn)
    return False


def upload_to_aws(local_file, bucket, s3_file):
    s3 = boto3.client(
        "s3",
        aws_access_key_id="AKIAZUXRKAEIJ6K4QOPL",
        aws_secret_access_key="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",
    )
    try:
        s3.upload_file(local_file, bucket, s3_file)
    except FileNotFoundError:
        print("The file was not found")
    except NoCredentialsError:
        print("Credentials not available")


def check_isbn_exists(isbn):
    try:
        return Livre.objects.get(isbn=isbn)
    except Livre.DoesNotExist:
        return False


def uploadThis(placeholderpath, bucket):
    for tpl in liste_isbn:
        (id, isbn) = tpl
        if os.path.isdir(os.path.join(placeholderpath, isbn)):
            for picture in os.listdir(os.path.join(placeholderpath, isbn)):
                # isbn, file_extension = os.path.splitext(picture)
                imgpath = os.path.join(placeholderpath, isbn, picture)
                # print(imgpath)
                # os.chdir(imgpath)
                if os.path.isfile(imgpath):
                    try:
                        book_instance = check_isbn_exists(isbn)
                        img = Image.open(imgpath)  # open the image file
                        img.verify()  # verify that it is, in fact an image
                        isbn, file_extension = os.path.splitext(imgpath)
                        s3_file = id + file_extension
                        # print(s3_file)
                        time.sleep(2)
                        # upload_to_aws(imgpath, bucket, s3_file)
                        file_instance = File(open(imgpath, "rb"), name=s3_file)
                        image_instance = ImageLivre(
                            livre=book_instance,
                            image=file_instance,
                            alt=f"Book import image for isbn: {isbn}",
                        )
                        image_instance.save()
                    except (IOError, SyntaxError) as e:
                        print(f"error: {imgpath}")
                else:
                    print(f"error2: {imgpath}")
        else:
            print(f"{isbn} failed to download")
            imgpath = "/home/ubuntu/Pictures/anto.png"
            img = Image.open(imgpath)  # open the image file
            img.verify()  # verify that it is, in fact an image
            isbn, file_extension = os.path.splitext(imgpath)
            s3_file = id + file_extension
            # print(s3_file)
            time.sleep(2)
            upload_to_aws(imgpath, bucket, s3_file)


def create_connection():
    return psycopg2.connect(
        host="127.0.0.1",
        database="",
        user="",
        password="",
    )


def select_all_books(conn):
    cur = conn.cursor()
    cur.execute("SELECT isbn FROM core_livre")
    rows = cur.fetchall()
    liste_isbn = []
    for row in rows:
        liste_isbn.append(row[0])
    return liste_isbn


def main():
    logging.basicConfig(filename="notfound.log", level=logging.ERROR)
    conn = create_connection()
    # liste_isbn = select_all_books(conn)
    url = "https://fr.search.yahoo.com/"
    bucket = "bookshop-images-f1492f08-f236-4a55-afb7-70ded209cb27"

    driver = make_webdriver()

    placeholderpath = "/home/ubuntu/Dev/ecom/ecom/frontend/static/thumbnails"

    uploadThis(placeholderpath, bucket)
    # for i, tpl in enumerate(liste_isbn):
    #     description = ""
    #     url = "https://yahoo.com.br/"
    #     (id, isbn) = tpl
    #     # if shouldDownloaded(placeholderpath, isbn):
    #     #     description = grab_from_wook(url, str(isbn), driver)
    #     #     if description is None or len(description) == 0:
    #     #         description = grab_from_bertrand(url, str(isbn), driver)
    #     if description is None or len(description) == 0:
    #         description = grab_from_amazon(url, str(isbn), driver)
    #         if description is None or len(description) == 0:
    #             description = grab_from_decitre(url, str(isbn), driver)
    #             if description is None or len(description) == 0:
    #                 description = grab_from_eyrolles(url, str(isbn), driver)
    #                 if description is None or len(description) == 0:
    #                     description = grab_from_franceloisir(url, str(isbn), driver)
    #                     if description is None or len(description) == 0:
    #                         description = grab_from_amazoncom(url, str(isbn), driver)
    #     if len(description) != 0:
    #         if description[0] != "h":
    #             download_it(
    #                 str(isbn),
    #                 description.replace("www.", "http://www."),
    #                 placeholderpath,
    #             )
    #         else:
    #             download_it(str(isbn), description, placeholderpath)

    #         print(f"{isbn} saved on fs")
    #     else:
    #         logging.error(f"{isbn} has no img")
    #         print(f"{round(i+1 / len(liste_isbn))}%")


if __name__ == "__main__":
    main()
